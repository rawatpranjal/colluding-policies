{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333806f-ba98-4a70-b5db-af5aa4cd5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_rewards(agent_action, opponent_action):\n",
    "    reward_matrix = np.array([[3, 0], [5, 1]])\n",
    "    agent_reward = reward_matrix[agent_action, opponent_action]\n",
    "    opponent_reward = reward_matrix[opponent_action, agent_action]\n",
    "    return agent_reward, opponent_reward\n",
    "\n",
    "def run_reinforce(num_steps, num_episodes, learning_rate, agent_p1, agent_p2, opponent_p1, opponent_p2, initial_agent_state, initial_opponent_state):\n",
    "    # Define the policies for the agent and the opponent with parametrized initial conditions\n",
    "    agent_policy = np.array([[agent_p1, 1-agent_p1], [agent_p2, 1-agent_p2]])\n",
    "    opponent_policy = np.array([[opponent_p1, 1-opponent_p1], [opponent_p2, 1-opponent_p2]])\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        agent_state = initial_agent_state\n",
    "        opponent_state = initial_opponent_state\n",
    "\n",
    "        agent_episode_cooperation = 0\n",
    "        agent_episode_actions = []\n",
    "        opponent_episode_actions = []\n",
    "\n",
    "        for i in range(num_steps):\n",
    "            # Agent's action\n",
    "            agent_action = np.random.choice(2, p=agent_policy[agent_state])\n",
    "            agent_episode_actions.append(agent_action)\n",
    "\n",
    "            # Opponent's action\n",
    "            opponent_action = np.random.choice(2, p=opponent_policy[opponent_state])\n",
    "            opponent_episode_actions.append(opponent_action)\n",
    "\n",
    "            # Update states\n",
    "            agent_state = opponent_action\n",
    "            opponent_state = agent_action\n",
    "\n",
    "            # Compute rewards based on the actions of both agents\n",
    "            agent_reward, opponent_reward = compute_rewards(agent_action, opponent_action)\n",
    "\n",
    "            # Update episode cooperation count\n",
    "            if agent_action == 0:\n",
    "                agent_episode_cooperation += 1\n",
    "\n",
    "        # Update the policies using REINFORCE update rule\n",
    "        for t in range(len(agent_episode_actions)):\n",
    "            agent_action_t = agent_episode_actions[t]\n",
    "            opponent_action_t = opponent_episode_actions[t]\n",
    "\n",
    "            agent_policy[agent_state][agent_action_t] += learning_rate * agent_episode_cooperation\n",
    "            opponent_policy[opponent_state][opponent_action_t] += learning_rate * agent_episode_cooperation  # The opponent's policy uses the agent's cooperation count\n",
    "\n",
    "            # Normalize the policies to make sure they are valid probability distributions\n",
    "            agent_policy[agent_state] /= np.sum(agent_policy[agent_state])\n",
    "            opponent_policy[opponent_state] /= np.sum(opponent_policy[opponent_state])\n",
    "\n",
    "        # Calculate and return the cooperation rate for the last episode\n",
    "        if episode == num_episodes - 1:\n",
    "            cooperation_rate = agent_episode_cooperation / num_steps\n",
    "            return cooperation_rate\n",
    "\n",
    "# Set up a grid of hyperparameters for random sampling\n",
    "param_grid = {\n",
    "    \"num_steps\": [10, 50, 100, 200, 500, 1000],\n",
    "    \"num_episodes\": [100, 200, 500, 1000],\n",
    "    \"learning_rate\": [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"agent_p1\": [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "    \"agent_p2\": [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "    \"opponent_p1\": [0.1, 0.25, 0.5, 0.75, 0.9], \n",
    "    \"opponent_p2\": [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "    \"initial_agent_state\": [0, 1],  # Initial state for agent\n",
    "    \"initial_opponent_state\": [0, 1]  # Initial state for opponent\n",
    "}\n",
    "\n",
    "# Randomly sample from the parameter grid\n",
    "num_samples = 90  # Adjust the number of samples as needed\n",
    "results = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    print(_)\n",
    "    params = {key: np.random.choice(values) for key, values in param_grid.items()}\n",
    "    cooperation_rate = run_reinforce(**params)\n",
    "    params[\"Cooperation Rate\"] = cooperation_rate  # Add the final cooperation rate to the parameters\n",
    "    results.append(params)  # Store the hyperparameters and the final cooperation rate\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
